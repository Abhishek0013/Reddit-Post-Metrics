{"cells":[{"cell_type":"markdown","source":["**Import Libraries**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import isnull, when, count, col,countDistinct,regexp_replace,lower,monotonically_increasing_id, hour, date_format, dayofweek, length, size, split, avg\nfrom pyspark.sql.window import Window\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.ml.feature import Word2Vec\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.sql.types import IntegerType,StringType,DoubleType,LongType,DecimalType,FloatType,ArrayType, TimestampType\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfrom math import cos, sin, pi\n\nimport sparknlp\nfrom sparknlp.pretrained import PretrainedPipeline\n!pip install googletrans\nfrom googletrans import Translator"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["**Read in Files - note media_embed and secure_media_embed do not load for some reason but they are both null (looking at the CSV file)**"],"metadata":{}},{"cell_type":"code","source":["# read in JSON data - train\ndf = spark.read.json(\"/FileStore/tables/RS_v2_2006_03\")\n\n# read in JSON data - test\ndf_test = spark.read.json(\"/FileStore/tables/RS_v2_2006_04\")\n\n# read in JSON data - oot\ndf_oot = spark.read.json(\"/FileStore/tables/RS_v2_2006_05\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["**Add Index (monotically increasing ID)**"],"metadata":{}},{"cell_type":"code","source":["# do for train dataset\ndf = df.withColumn(\"idx\", monotonically_increasing_id())\n# then since the id is increasing but not consecutive, it means you can sort by it, so you can use the `row_number`\ndf.createOrReplaceTempView('df_temp')\ndf = spark.sql('select *, row_number() over (order by \"idx\") as index from df_temp')\n\n# do for test dataset\ndf_test = df_test.withColumn(\"idx\", monotonically_increasing_id())\n# then since the id is increasing but not consecutive, it means you can sort by it, so you can use the `row_number`\ndf_test.createOrReplaceTempView('df_temp')\ndf_test = spark.sql('select *, row_number() over (order by \"idx\") as index from df_temp')\n\n# do for oot dataset\ndf_oot = df_oot.withColumn(\"idx\", monotonically_increasing_id())\n# then since the id is increasing but not consecutive, it means you can sort by it, so you can use the `row_number`\ndf_oot.createOrReplaceTempView('df_temp')\ndf_oot = spark.sql('select *, row_number() over (order by \"idx\") as index from df_temp')"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["**Prep data filling null values with NaN (Simon) copy of dataframe is made**"],"metadata":{}},{"cell_type":"code","source":["def prep_data (df):#select columns that will be used\n  select_list=['index', 'idx',\n  'author',\n  'author_flair_text_color',\n  'author_flair_type',\n  'brand_safe',\n  'created_utc',\n  'domain',\n  'is_crosspostable',\n  'no_follow',\n  'num_comments',\n  'over_18',\n  'parent_whitelist_status',\n  'permalink',\n  'score',\n  'subreddit',\n  'subreddit_type',\n  'title',\n  'url',\n  'whitelist_status']\n  df2=df.select(*select_list).withColumnRenamed('score','label') # change target variable name to label\n  df2=df2.fillna('NaN') #fill null value with NaN\n  return df2\n\ntrain=prep_data(df)\ntest=prep_data(df_test)\noot = prep_data(df_oot)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["**Clean text columns that need to be vectorized (Simon)**"],"metadata":{}},{"cell_type":"code","source":["def clean_text(c): #clean text columns that need to be vectorized (tfidf)\n  c = lower(c)\n  #c = regexp_replace(c, \"\\\\s+\",\"\")#remove all white space \n  c = regexp_replace(c, \"[^a-zA-Z0-9]\",\" \")#remove all non-alphanumeric characters\n  c = regexp_replace(c, 'www|com|http',\" \")# \n  c = regexp_replace(c, \"\\d+\", \"\")#remove digital numbers\n  return c\n\n# Clean Text\ntrain = train.withColumn('domain',clean_text(col(\"domain\")))\\\n         .withColumn('permalink',clean_text(col(\"permalink\")))\\\n         .withColumn('subreddit',clean_text(col(\"subreddit\")))\\\n         .withColumn('title',clean_text(col(\"title\")))\\\n         .withColumn('url',clean_text(col(\"url\")))\ntest = test.withColumn('domain',clean_text(col(\"domain\")))\\\n         .withColumn('permalink',clean_text(col(\"permalink\")))\\\n         .withColumn('subreddit',clean_text(col(\"subreddit\")))\\\n         .withColumn('title',clean_text(col(\"title\")))\\\n         .withColumn('url',clean_text(col(\"url\")))\noot = oot.withColumn('domain',clean_text(col(\"domain\")))\\\n         .withColumn('permalink',clean_text(col(\"permalink\")))\\\n         .withColumn('subreddit',clean_text(col(\"subreddit\")))\\\n         .withColumn('title',clean_text(col(\"title\")))\\\n         .withColumn('url',clean_text(col(\"url\")))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["**Pipeline for TF-IDF (Simon)**"],"metadata":{}},{"cell_type":"code","source":["def pipeline (df):\n  cat_features= [t[0] for t in df.dtypes if t[1] == 'string'] #select all string feature\n  cat_features= [c for c in cat_features if c not in text_features] #remove text feature so only have catigorical feature left\n  boolean_features=[t[0] for t in df.dtypes if t[1] == 'boolean'] #select all boolean features \n  for col in boolean_features:  #convert boolean to int : for FvsT ->0vs1\n      df=df.withColumn(col,df[col].cast(IntegerType()))\n      numeric_features=[t[0] for t in df.dtypes if t[1].startswith(('int', 'bigint'))] #select all numerical feature \n  stages=[]\n  for categoricalCol in cat_features:#apply onehot encoder to all cat features- binary matrix \n      stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n      encoder = OneHotEncoder(inputCol=categoricalCol + 'Index', outputCol=categoricalCol + \"classVec\")\n      stages += [stringIndexer,encoder]\n  \n  tfidf_features=[]#apply tfidf to all text columns\n  for textCol in text_features:   \n      #print(\"1\",textCol)\n      tokenizer = Tokenizer(inputCol=textCol, outputCol=textCol+\"token\") \n      remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=textCol+\"words\")\n      #print(\"2\",textCol)\n      # Count the words in a document\n      hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=textCol+\"rawFeatures\")\n      #print(hashingTF.getOutputCol())\n      # Build the idf model and transform the original token frequencies into their tf-idf counterparts\n      idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=textCol+\"tfidf\")\n      #print(\"4\",textCol)\n      tfidf_features.append(textCol+\"tfidf\")\n      stages += [tokenizer,remover,hashingTF,idf]\n  #print(tfidf_features)\n      \n  #assemblerInputs=[c + 'classVec' for c in cat_features ] + numeric_features +tfidf_features\n  assemblerInputs=tfidf_features\n  #assemblerInputs.remove('label')\n  #print(assemblerInputs)\n  assembler=VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n  #print(assembler)\n  stages+=[assembler]\n  pipeline2 = Pipeline(stages = stages)\n  #print (type(stages[13]))\n  pip_model = pipeline2.fit(df)\n  df_input = pip_model.transform(df)\n  return df_input\n\ntext_features=['domain','permalink','subreddit','title','url'] \ntrain_input=pipeline(train)\ntest_input=pipeline(test)\noot_input=pipeline(oot)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["**Select Simon's rows only (separate DF) - convert is_crosspostable from boolean to integer, tf-idf for domain**"],"metadata":{}},{"cell_type":"code","source":["#train\ndf_simon_train = train_input.withColumn(\"is_crosspostable_encoded\",train_input.is_crosspostable)\ndf_simon_train = df_simon_train.select(\"idx\",\"index\",\"is_crosspostable_encoded\",\"domaintfidf\")\ndf_simon_train.show(4,False)\n\n#test\ndf_simon_test = test_input.withColumn(\"is_crosspostable_encoded\",test_input.is_crosspostable)\ndf_simon_test = df_simon_test.select(\"idx\",\"index\",\"is_crosspostable_encoded\",\"domaintfidf\")\ndf_simon_test.show(4,False)\n\n#oot\ndf_simon_oot = oot_input.withColumn(\"is_crosspostable_encoded\",oot_input.is_crosspostable)\ndf_simon_oot = df_simon_oot.select(\"idx\",\"index\",\"is_crosspostable_encoded\",\"domaintfidf\")\ndf_simon_oot.show(4,False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# add these two columns to the main dataframe\n#train\ndf = (df.alias('df').join(df_simon_train.alias('df_simon_train'),\n                         on = df['index'] == df_simon_train['index'],\n                         how = 'left')\n     .select('df.*','is_crosspostable_encoded','domaintfidf')\n     )\n\n#test\ndf_test = (df_test.alias('df_test').join(df_simon_test.alias('df_simon_test'),\n                         on = df_test['index'] == df_simon_test['index'],\n                         how = 'left')\n     .select('df_test.*','is_crosspostable_encoded','domaintfidf')\n     )\n\n#oot\ndf_oot = (df_oot.alias('df_oot').join(df_simon_oot.alias('df_simon_oot'),\n                         on = df_oot['index'] == df_simon_oot['index'],\n                         how = 'left')\n     .select('df_oot.*','is_crosspostable_encoded','domaintfidf')\n     )"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display (df_simon_test)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["**Transformations - add timestamp field**"],"metadata":{}},{"cell_type":"code","source":["# Convert created_utc field, which is epoch time to date time YYYY-MM-DD HH:SS format\n\n# do for train dataset\ndf = df.withColumn(\"timestamp\", df[\"created_utc\"].cast(TimestampType()))\ndisplay(df)\n\n# do for test dataset\ndf_test = df_test.withColumn(\"timestamp\", df_test[\"created_utc\"].cast(TimestampType()))\n\n# do for oot dataset\ndf_oot = df_oot.withColumn(\"timestamp\", df_oot[\"created_utc\"].cast(TimestampType()))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**Register tables for SQL Queries**"],"metadata":{}},{"cell_type":"code","source":["#register df as temptable for sql queries\ndf.registerTempTable('reddit_posts')\n#register df as temptable for sql queries\ndf_test.registerTempTable('reddit_posts_test')\n#register df as temptable for sql queries\ndf_oot.registerTempTable('reddit_posts_oot')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["**Add Features**"],"metadata":{}},{"cell_type":"markdown","source":["**total_subreddit_posts** - If there are a lot of posts in that subreddit during the month, that is indicative that the subreddit has an active community, which may mean higher potential of the subreddit to attain high scores. The converse may be true as well."],"metadata":{}},{"cell_type":"code","source":["# do for train dataset\ndf = sqlContext.sql(\"SELECT *, count(index) OVER (PARTITION BY subreddit) total_subreddit_posts FROM reddit_posts \")\ndf.registerTempTable(\"reddit_posts\")\n\n# do for test dataset\ndf_test = sqlContext.sql(\"\"\"\nSELECT *, count(index) OVER (PARTITION BY subreddit) total_subreddit_posts\nFROM reddit_posts_test\n\"\"\")\ndf_test.registerTempTable(\"reddit_posts_test\")\n\n# do for oot dataset\ndf_oot = sqlContext.sql(\"\"\"\nSELECT *, count(index) OVER (PARTITION BY subreddit) total_subreddit_posts\nFROM reddit_posts_oot\n\"\"\")\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["**subreddit_hotness** - Perhaps if there are a lot of posts in the subreddit in the past week, that could be indicative that the subreddit is currently hot and may have higher views and inturn perhaps higher scores."],"metadata":{}},{"cell_type":"code","source":["# do for train dataset\ndf = sqlContext.sql(\"\"\"\nSELECT *, \ncount(score) OVER (\n        PARTITION BY subreddit \n        ORDER BY timestamp \n        RANGE BETWEEN INTERVAL 7 DAYS PRECEDING AND CURRENT ROW\n         ) AS subreddit_hotness\n\nFROM reddit_posts\n\"\"\")\n\ndf.registerTempTable(\"reddit_posts\")\n\ndf_test = sqlContext.sql(\"\"\"\nSELECT *, \ncount(score) OVER (\n        PARTITION BY subreddit \n        ORDER BY timestamp \n        RANGE BETWEEN INTERVAL 7 DAYS PRECEDING AND CURRENT ROW\n         ) AS subreddit_hotness\n\nFROM reddit_posts_test\n\"\"\")\n\ndf_test.registerTempTable(\"reddit_posts_test\")\n\ndf_oot = sqlContext.sql(\"\"\"\nSELECT *, \ncount(score) OVER (\n        PARTITION BY subreddit \n        ORDER BY timestamp \n        RANGE BETWEEN INTERVAL 7 DAYS PRECEDING AND CURRENT ROW\n         ) AS subreddit_hotness\n\nFROM reddit_posts_oot\n\"\"\")\n\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["**subreddit_author_count** - Perhaps if there are a lot of contributing authors in the subreddit, that could be indicative that the subreddit is popular and may have higher views and inturn perhaps higher scores."],"metadata":{}},{"cell_type":"code","source":["#create a count column of authors\ngr = df \\\n        .groupBy(\"subreddit\", \"author\").count().groupBy(\"subreddit\").count() \\\n        .withColumnRenamed(\"subreddit\", \"subreddit_gr\") \\\n        .withColumnRenamed(\"count\", \"subreddit_author_count\")\n\n#join using subreddit as id\ndf = df.join(gr, df['subreddit']==gr['subreddit_gr']).drop(\"subreddit_gr\")\ndf.registerTempTable(\"reddit_posts\")\n\n#create a count column of authors\ngr = df_test \\\n        .groupBy(\"subreddit\", \"author\").count().groupBy(\"subreddit\").count() \\\n        .withColumnRenamed(\"subreddit\", \"subreddit_gr\") \\\n        .withColumnRenamed(\"count\", \"subreddit_author_count\")\n\n#join using subreddit as id\ndf_test = df_test.join(gr, df_test['subreddit']==gr['subreddit_gr']).drop(\"subreddit_gr\")\ndf_test.registerTempTable(\"reddit_posts_test\")\n\n#create a count column of authors\ngr = df_oot \\\n        .groupBy(\"subreddit\", \"author\").count().groupBy(\"subreddit\").count() \\\n        .withColumnRenamed(\"subreddit\", \"subreddit_gr\") \\\n        .withColumnRenamed(\"count\", \"subreddit_author_count\")\n\n#join using subreddit as id\ndf_oot = df_oot.join(gr, df_oot['subreddit']==gr['subreddit_gr']).drop(\"subreddit_gr\")\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["**total_author_score_ntile** - if an author has scored a lot of upvotes in the past, it is possible that the author is influential in the community, and may likely garner more upvotes in the future. In this, we first calculate the  cumulative score obtained for each author. Then, we use NTILE function to divide the scores into 10 groups. We will later onehotencode each cateogory. We note that author with the highest score is when author= [deleted]. The underlying assumption in our model for these posts is that when the author is [deleted], this is meaningful in terms of the overall score of the post.  **data leakage-didn't use**"],"metadata":{}},{"cell_type":"code","source":["df_1 = sqlContext.sql(\"\"\"\n\nSELECT index, author, score, total_author_score, NTILE(10) over (order by total_author_score DESC) total_author_score_ntile\nFROM\n(\n    SELECT index, author, score, sum(score) OVER (PARTITION BY author) total_author_score\n    FROM reddit_posts\n)\nORDER BY total_author_score DESC\n\"\"\")\ndf_1.registerTempTable(\"reddit_posts_author\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index, total_author_score_ntile \n  FROM reddit_posts_author\n  ) t\nJOIN reddit_posts ON t.author_index = reddit_posts.index\nORDER BY index ASC\n\"\"\")\ndf.registerTempTable(\"reddit_posts\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# do for test\ndf_1_test = sqlContext.sql(\"\"\"\nSELECT index, author, score, total_author_score, NTILE(10) over (order by total_author_score DESC) total_author_score_ntile\nFROM\n(\n    SELECT index, author, score, sum(score) OVER (PARTITION BY author) total_author_score\n    FROM reddit_posts_test\n)\nORDER BY total_author_score DESC\n\"\"\")\ndf_1_test.registerTempTable(\"reddit_posts_author_test\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# do for test\ndf_test = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index, total_author_score_ntile \n  FROM reddit_posts_author_test\n  ) t\nJOIN reddit_posts_test ON t.author_index = reddit_posts_test.index\nORDER BY index ASC\n\"\"\")\n\ndf_test.registerTempTable(\"reddit_posts_test\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# do for oot\n\ndf_1_oot = sqlContext.sql(\"\"\"\nSELECT index, author, score, total_author_score, NTILE(10) over (order by total_author_score DESC) total_author_score_ntile\nFROM\n(\n    SELECT index, author, score, sum(score) OVER (PARTITION BY author) total_author_score\n    FROM reddit_posts_oot\n)\nORDER BY total_author_score DESC\n\"\"\")\ndf_1_oot.registerTempTable(\"reddit_posts_author_oot\")\n\ndf_oot = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index, total_author_score_ntile \n  FROM reddit_posts_author_oot\n  ) t\nJOIN reddit_posts_oot ON t.author_index = reddit_posts_oot.index\nORDER BY index ASC\n\"\"\")\n\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["**avg_author_score_ntile** - this is similar to total_author_score_ntile, except we take the average score of the author instead of the total score  **data leakage-didn't use**"],"metadata":{}},{"cell_type":"code","source":["df_1 = sqlContext.sql(\"\"\"\nSELECT index, author, score, avg_author_score, NTILE(10) over (order by avg_author_score DESC) avg_author_score_ntile\nFROM\n(\n    SELECT index, author, score, AVG(score) OVER (PARTITION BY author) avg_author_score\n    FROM reddit_posts\n)\nORDER BY avg_author_score DESC\n\"\"\")\ndf_1.registerTempTable(\"reddit_posts_author\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#edited to distinguish author_index from previous one\ndf = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index2, avg_author_score_ntile \n  FROM reddit_posts_author\n  ) t\nJOIN reddit_posts ON t.author_index2 = reddit_posts.index\nORDER BY index ASC\n\"\"\")\ndf.registerTempTable(\"reddit_posts\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# do for test\ndf_1_test = sqlContext.sql(\"\"\"\nSELECT index, author, score, avg_author_score, NTILE(10) over (order by avg_author_score DESC) avg_author_score_ntile\nFROM\n(\n    SELECT index, author, score, AVG(score) OVER (PARTITION BY author) avg_author_score\n    FROM reddit_posts_test\n)\nORDER BY avg_author_score DESC\n\"\"\")\ndf_1_test.registerTempTable(\"reddit_posts_author_test\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df_test = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index2, avg_author_score_ntile \n  FROM reddit_posts_author_test\n  ) t\nJOIN reddit_posts_test ON t.author_index2 = reddit_posts_test.index\nORDER BY index ASC\n\"\"\")\n\ndf_test.registerTempTable(\"reddit_posts_test\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# do for oot\ndf_1_oot = sqlContext.sql(\"\"\"\nSELECT index, author, score, avg_author_score, NTILE(10) over (order by avg_author_score DESC) avg_author_score_ntile\nFROM\n(\n    SELECT index, author, score, AVG(score) OVER (PARTITION BY author) avg_author_score\n    FROM reddit_posts_oot\n)\nORDER BY avg_author_score DESC\n\"\"\")\ndf_1_oot.registerTempTable(\"reddit_posts_author_oot\")\n\ndf_oot = spark.sql(\"\"\"\nSELECT * FROM \n  (\n  SELECT index as author_index2, avg_author_score_ntile \n  FROM reddit_posts_author_oot\n  ) t\nJOIN reddit_posts_oot ON t.author_index2 = reddit_posts_oot.index\nORDER BY index ASC\n\"\"\")\n\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["**Encode Columns**"],"metadata":{}},{"cell_type":"markdown","source":["**binned_score** - this is similar to total_author_score_ntile, except we take the average score of the author instead of the total score"],"metadata":{}},{"cell_type":"code","source":["df = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN score < 20 THEN 1 \n           WHEN score >= 20 and score  < 100 THEN 2\n           ELSE 3\n           END) binned_score\n           FROM reddit_posts\n\"\"\")\ndf.registerTempTable(\"reddit_posts\")"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["df_test = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN score < 20 THEN 1 \n           WHEN score >= 20 and score  < 100 THEN 2\n           ELSE 3\n           END) binned_score\n           FROM reddit_posts_test\n\"\"\")\n\ndf_test.registerTempTable(\"reddit_posts_test\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["df_oot = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN score < 20 THEN 1 \n           WHEN score >= 20 and score  < 100 THEN 2\n           ELSE 3\n           END) binned_score\n           FROM reddit_posts_oot\n\"\"\")\n\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["**Encode author_cakeday, author_flair_text_color, brand_safe**"],"metadata":{}},{"cell_type":"code","source":["#encode the author_cakeday column\ndf = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_cakeday = \"true\" THEN 1\n           ELSE 0\n           END) author_cakeday_encoded\n           FROM reddit_posts\n\"\"\")\ndf = df.drop(\"author_cakeday\")\ndf.registerTempTable(\"reddit_posts\")\n\n#encode the author_cakeday column in test\ndf_test = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_cakeday = \"true\" THEN 1\n           ELSE 0\n           END) author_cakeday_encoded\n           FROM reddit_posts_test\n\"\"\")\ndf_test = df_test.drop(\"author_cakeday\")\ndf_test.registerTempTable(\"reddit_posts_test\")\n\n#encode the author_cakeday column in oot\ndf_oot = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_cakeday = \"true\" THEN 1\n           ELSE 0\n           END) author_cakeday_encoded\n           FROM reddit_posts_oot\n\"\"\")\ndf_oot = df_oot.drop(\"author_cakeday\")\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# author_flair_text_color - train\ndf = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_flair_text_color = \"dark\" THEN 1\n           ELSE 0\n           END) author_flair_text_color_encoded\n           FROM reddit_posts\n\"\"\")\ndf = df.drop(\"author_flair_text_color\")\ndf.registerTempTable(\"reddit_posts\")\n\n# test\ndf_test = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_flair_text_color = \"dark\" THEN 1\n           ELSE 0\n           END) author_flair_text_color_encoded\n           FROM reddit_posts_test\n\"\"\")\ndf_test = df_test.drop(\"author_flair_text_color\")\ndf_test.registerTempTable(\"reddit_posts_test\")\n\n# oot\ndf_oot = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN author_flair_text_color = \"dark\" THEN 1\n           ELSE 0\n           END) author_flair_text_color_encoded\n           FROM reddit_posts_oot\n\"\"\")\ndf_oot = df_oot.drop(\"author_flair_text_color\")\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# brand_safe\ndf = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN brand_safe = \"true\" THEN 1\n           ELSE 0\n           END) brand_safe_encoded\n           FROM reddit_posts\n\"\"\")\ndf = df.drop(\"brand_safe\")\ndf.registerTempTable(\"reddit_posts\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["df_test = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN brand_safe = \"true\" THEN 1\n           ELSE 0\n           END) brand_safe_encoded\n           FROM reddit_posts_test\n\"\"\")\ndf_test = df_test.drop(\"brand_safe\")\ndf_test.registerTempTable(\"reddit_posts_test\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["df_oot = spark.sql(\"\"\"\n           SELECT *, (CASE \n           WHEN brand_safe = \"true\" THEN 1\n           ELSE 0\n           END) brand_safe_encoded\n           FROM reddit_posts_oot\n\"\"\")\ndf_oot = df_oot.drop(\"brand_safe\")\ndf_oot.registerTempTable(\"reddit_posts_oot\")"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["**Encoding (Abhishek)** - no_follow, over_18"],"metadata":{}},{"cell_type":"code","source":["t_cols=['no_follow','over_18']\nfor c in t_cols:\n  df=df.withColumn(c + \"_enc\",when(col(c)==True,1).when(col(c)==False,0))\n  df_test=df_test.withColumn(c + \"_enc\",when(col(c)==True,1).when(col(c)==False,0))\n  df_oot=df_oot.withColumn(c + \"_enc\",when(col(c)==True,1).when(col(c)==False,0))\ndisplay(df_test)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["**Subreddit Binning (Chris)** - those with counts < 30 put under \"other\""],"metadata":{}},{"cell_type":"code","source":["# use Sahil's column that has total posts by subreddit\n\ndef subreddit_binning (subreddit, value):\n  if value < 30:\n    return 'other'\n  else:\n    return subreddit\n\nsubreddit_udf = udf(subreddit_binning,StringType())\ndf = df.withColumn(\"subreddit_rev\",subreddit_udf('subreddit','total_subreddit_posts'))\n#df.sort('total_subreddit_posts',ascending = True).show(10)\n\ndf_test = df_test.withColumn(\"subreddit_rev\",subreddit_udf('subreddit','total_subreddit_posts'))\n\ndf_oot = df_oot.withColumn(\"subreddit_rev\",subreddit_udf('subreddit','total_subreddit_posts'))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["**Encode Creation Day of Week and Hour (Chris) as cyclical ordinal value**"],"metadata":{}},{"cell_type":"code","source":["#udf for sine and cosine function\ndef time_conversion_sin (hour):\n  hour_sin = sin(2*pi*hour / 23)\n  return hour_sin\n  \ntime_conv_sin_udf = udf(time_conversion_sin,FloatType())  \n\ndef time_conversion_cos (hour):\n  hour_cos = cos(2*pi*hour / 23)\n  return hour_cos\n  \ntime_conv_cos_udf = udf (time_conversion_cos, FloatType())"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# add creation hour and Day of Week to dataframe\ndf = df.withColumn(\"create_hour\", hour(col(\"timestamp\"))).withColumn(\"DOW\",date_format(col(\"timestamp\"),\"E\")).withColumn(\"DOW_value\",dayofweek(col(\"timestamp\"))).withColumn(\"created_hour_sin\",time_conv_sin_udf(col(\"create_hour\"))).withColumn(\"created_hour_cos\",time_conv_cos_udf(col(\"create_hour\")))\n\n# do same thing for test\ndf_test = df_test.withColumn(\"create_hour\", hour(col(\"timestamp\"))).withColumn(\"DOW\",date_format(col(\"timestamp\"),\"E\")).withColumn(\"DOW_value\",dayofweek(col(\"timestamp\"))).withColumn(\"created_hour_sin\",time_conv_sin_udf(col(\"create_hour\"))).withColumn(\"created_hour_cos\",time_conv_cos_udf(col(\"create_hour\")))\n\n# do same thing for oot\ndf_oot = df_oot.withColumn(\"create_hour\", hour(col(\"timestamp\"))).withColumn(\"DOW\",date_format(col(\"timestamp\"),\"E\")).withColumn(\"DOW_value\",dayofweek(col(\"timestamp\"))).withColumn(\"created_hour_sin\",time_conv_sin_udf(col(\"create_hour\"))).withColumn(\"created_hour_cos\",time_conv_cos_udf(col(\"create_hour\")))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["**Impute null values for whitelist_status & create index values (Siddhant)**"],"metadata":{}},{"cell_type":"code","source":["# train - consolidate with other StringIndexers\ndf = df.fillna({\"whitelist_status\":0})\n#indexer = StringIndexer(inputCol=\"whitelist_status\", outputCol=\"whitelist_statusIndex\")\n#df = indexer.fit(df).transform(df)\n#df = df.drop(\"whitelist_status\")\n\n#test\ndf_test = df_test.fillna({\"whitelist_status\":0})\n\n#oot\ndf_oot = df_oot.fillna({\"whitelist_status\":0})"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["**Text Translation and Cleansing for Title**"],"metadata":{}},{"cell_type":"code","source":["def trans(x):\n  translator=Translator()\n  return translator.translate(x, dest=\"en\").text\n\nlation = udf(trans)\nspark.udf.register(\"lation\", lation)\n\ndef clean_str(x):\n  punc = ''';:,.|'''\n  for ch in x:\n    if ch in punc:\n      x = x.replace(ch, '')\n  return x\nclean = udf(clean_str)\nspark.udf.register(\"clean\", clean)\n\n# make a temp dataframe using input column labelled as text\ndf_text_title = df.select(\"title\")\ndf_text_title = df_text_title.withColumn(\"translated text\", lation(\"title\"))\ndf_text_title1 = df_text_title.select(\"translated text\")\ndf_text_title1 = df_text_title1.withColumn(\"text\", clean(\"translated text\"))\ndf_text_title2 = df_text_title1.select(\"text\")\ndisplay(df_text_title2)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# testing\ndf_test_text_title = df_test.select(\"title\")\ndf_test_text_title = df_test_text_title.withColumn(\"translated text\", lation(\"title\"))\ndf_test_text_title1 = df_test_text_title.select(\"translated text\")\ndf_test_text_title1 = df_test_text_title1.withColumn(\"text\", clean(\"translated text\"))\ndf_test_text_title2 = df_test_text_title1.select(\"text\")\ndisplay(df_test_text_title2.take(2))"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# oot\ndf_oot_text_title = df_oot.select(\"title\")\ndf_oot_text_title = df_oot_text_title.withColumn(\"translated text\", lation(\"title\"))\ndf_oot_text_title1 = df_oot_text_title.select(\"translated text\")\ndf_oot_text_title1 = df_oot_text_title1.withColumn(\"text\", clean(\"translated text\"))\ndf_oot_text_title2 = df_oot_text_title1.select(\"text\")\ndisplay(df_oot_text_title2.take(2))"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["**Sentiment Analysis**"],"metadata":{}},{"cell_type":"code","source":["pipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n\n# Transform 'data' and store output in a new 'annotations_df' dataframe\ndf_transformed = pipeline.transform(df_text_title2)\n\ndf_new = df_transformed.select(\"sentiment.result\", \"text\")\ndisplay(df_new)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# test dataset\npipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n\n# Transform 'data' and store output in a new 'annotations_df' dataframe\ndf_transformed_test = pipeline.transform(df_test_text_title2)\n\ndf_new_test = df_transformed_test.select(\"sentiment.result\", \"text\")\ndisplay(df_new_test)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["# oot dataset\npipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n\n# Transform 'data' and store output in a new 'annotations_df' dataframe\ndf_transformed_oot = pipeline.transform(df_oot_text_title2)\n\ndf_new_oot = df_transformed_oot.select(\"sentiment.result\", \"text\")\ndisplay(df_new_oot)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n#join temperorary dataframe to original dataframe\ndf_new = df_new.withColumn(\"id_1\", monotonically_increasing_id())\ndf_new.createOrReplaceTempView('df_temp1')\ndf_new = spark.sql('select *, row_number() over (order by \"idx\") as index1 from df_temp1').drop('id_1')\ndf_full = df.join(df_new, df.index == df_new.index1).orderBy(df.index).drop(\"index1\", \"title\")\n#display(df_full)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["#cache statement, do all feature engineering transformations, then perform the display action in the end\ndf_full.cache()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["# test dataset\n#join temperorary dataframe to original dataframe\ndf_new_test = df_new_test.withColumn(\"id_1\", monotonically_increasing_id())\ndf_new_test.createOrReplaceTempView('df_temp1')\ndf_new_test = spark.sql('select *, row_number() over (order by \"idx\") as index1 from df_temp1').drop('id_1')\n\ndf_full_test = (df_test.alias('df_test').join(df_new_test.alias('df_new_test'),\n                         on = df_test['index'] == df_new_test['index1'],\n                         how = 'left')\n     .select('df_test.*','result','text')\n     )\n\n# cache\ndf_full_test.cache()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# oot dataset\n#join temperorary dataframe to original dataframe\ndf_new_oot = df_new_oot.withColumn(\"id_1\", monotonically_increasing_id())\ndf_new_oot.createOrReplaceTempView('df_temp1')\ndf_new_oot = spark.sql('select *, row_number() over (order by \"idx\") as index1 from df_temp1').drop('id_1')\n\ndf_full_oot = (df_oot.alias('df_oot').join(df_new_oot.alias('df_new_oot'),\n                         on = df_oot['index'] == df_new_oot['index1'],\n                         how = 'left')\n     .select('df_oot.*','result','text')\n     )\n\n# cache\ndf_full_oot.cache()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["**<span style=\"color:red\">If Databricks stops working or you receive errors, use the backup dataframes, but reverse the command (df_full = df_full_bk), etc. and run from this point onwards </span>**"],"metadata":{}},{"cell_type":"code","source":["#create backups\ndf_full_bk = df_full\ndf_full_test_bk = df_full_test\ndf_full_oot_bk = df_full_oot"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["df_full = df_full.withColumn(\"length_title\", length(\"text\"))\ndf_full = df_full.withColumn('titlewordCount', size(split(col('text'), ' '))) #https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\ndf_full = df_full.withColumn('url length', length(split(col('url'), '//').getItem(1)))\n#display(df_full)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["df_full_test = df_full_test.withColumn(\"length_title\", length(\"text\"))\ndf_full_test = df_full_test.withColumn('titlewordCount', size(split(col('text'), ' '))) #https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\ndf_full_test = df_full_test.withColumn('url length', length(split(col('url'), '//').getItem(1)))"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["df_full_oot = df_full_oot.withColumn(\"length_title\", length(\"text\"))\ndf_full_oot = df_full_oot.withColumn('titlewordCount', size(split(col('text'), ' '))) #https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\ndf_full_oot = df_full_oot.withColumn('url length', length(split(col('url'), '//').getItem(1)))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["**Add flags if title has exclamation or question mark**"],"metadata":{}},{"cell_type":"code","source":["#new features\n\n#has exclamation mark in the end?\ndf_full = df_full.withColumn( \\\n          'has_exclamation_mark', when(col(\"text\").endswith('!') == True , 1) \\\n          .otherwise(0))\n\n#has question mark in the end?\ndf_full = df_full.withColumn( \\\n          'has_question_mark', when(col(\"text\").endswith('?') == True , 1) \\\n          .otherwise(0))"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# test data\ndf_full_test = df_full_test.withColumn('has_exclamation_mark', when(col(\"text\").endswith('!') == True , 1).otherwise(0))\ndf_full_test = df_full_test.withColumn('has_question_mark', when(col(\"text\").endswith('?') == True , 1).otherwise(0))"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["# oot data\ndf_full_oot = df_full_oot.withColumn('has_exclamation_mark', when(col(\"text\").endswith('!') == True , 1).otherwise(0))\ndf_full_oot = df_full_oot.withColumn('has_question_mark', when(col(\"text\").endswith('?') == True , 1).otherwise(0))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["def last(x):\n  return str(x).split('.')[-1]\n# String type output\n\nlast_udf = udf(last, StringType())\nspark.udf.register(\"last_udf\", last_udf)\ndf_full = df_full.withColumn(\"last\", last_udf('domain'))\ndf_full_test = df_full_test.withColumn(\"last\", last_udf('domain'))\ndf_full_oot = df_full_oot.withColumn(\"last\", last_udf('domain'))\n#display(df_full)"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["def protocol(x):\n  return str(x).split(\":\")[0]\n\n# String type output\nprotocol_udf = udf(protocol, StringType())\nspark.udf.register(\"protocol_udf\", protocol_udf)\ndf_full = df_full.withColumn(\"protocol\", protocol_udf('url'))\ndf_full_test = df_full_test.withColumn(\"protocol\", protocol_udf('url'))\ndf_full_oot = df_full_oot.withColumn(\"protocol\", protocol_udf('url'))\n#df_full = df_full.drop(\"subreddit_type\", \"url\", \"whitelist_status\", \"text\")\n#display(df_full)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["**Add Top 500 websites**"],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/top500Domains.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf500 = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n# display(df500)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["df500 = df500.withColumn(\"last_domain\", last_udf('Root Domain'))\nwindowSpec = Window.partitionBy(df500['last_domain'])\ndf500 = df500.withColumn(\"average da\", avg(df500['Domain Authority']).over(windowSpec)).orderBy('Rank')\n# display(df500)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["dflast = df500.select('last_domain', 'average da').distinct()\n# display(dflast)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["df_full = df_full.join(dflast, df_full.last == dflast.last_domain).drop('last_domain')\ndf_full = df_full.fillna({\"average da\": 1})\ndf_full_test = df_full_test.join(dflast, df_full_test.last == dflast.last_domain).drop('last_domain')\ndf_full_test = df_full_test.fillna({\"average da\": 1})\ndf_full_oot = df_full_oot.join(dflast, df_full_oot.last == dflast.last_domain).drop('last_domain')\ndf_full_oot = df_full_oot.fillna({\"average da\": 1})\n#display(df_full)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["# create backup again\ndf_full_bk = df_full\ndf_full_test_bk = df_full_test\ndf_full_oot_bk = df_full_oot"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["df_full = df_full.withColumn(\"result1\", df_full[\"result\"].getItem(1)).withColumn(\"result\", df_full[\"result\"].getItem(0))\n\ndf_full = df_full.fillna({\"result\":\"none\"})\nindexer = StringIndexer(inputCol=\"result\", outputCol=\"resultIndex\")\ndf_full = indexer.fit(df_full).transform(df_full)\n\nindexer1 = StringIndexer(inputCol=\"last\", outputCol=\"lastIndex\")\ndf_full = indexer1.fit(df_full).transform(df_full)\n\nindexer2 = StringIndexer(inputCol=\"protocol\", outputCol=\"protocolIndex\")\ndf_full = indexer2.fit(df_full).transform(df_full)\n\ndf_full = df_full.fillna({\"result1\":\"none\"})\nindexer3 = StringIndexer(inputCol=\"result1\", outputCol=\"result1Index\")\ndf_full = indexer3.fit(df_full).transform(df_full)\n\ndf_full = df_full.drop(\"result\", \"last\", \"protocol\", \"result1\")\n\n# display(df_full)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["df_full_test = df_full_test.withColumn(\"result1\", df_full_test[\"result\"].getItem(1)).withColumn(\"result\", df_full_test[\"result\"].getItem(0))\n\ndf_full_test = df_full_test.fillna({\"result\":\"none\"})\nindexer = StringIndexer(inputCol=\"result\", outputCol=\"resultIndex\")\ndf_full_test = indexer.fit(df_full_test).transform(df_full_test)\n\ndf_full_test = df_full_test.fillna({\"result1\":\"none\"})\n\npipeline = Pipeline(stages=[indexer1, indexer2, indexer3])\ndf_full_test = pipeline.fit(df_full_test).transform(df_full_test)\n\ndf_full_test = df_full_test.drop(\"result\", \"last\", \"protocol\", \"result1\")\n# display(df_full_test)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["df_full_oot = df_full_oot.withColumn(\"result1\", df_full_oot[\"result\"].getItem(1)).withColumn(\"result\", df_full_oot[\"result\"].getItem(0))\n\ndf_full_oot = df_full_oot.fillna({\"result\":\"none\"})\nindexer = StringIndexer(inputCol=\"result\", outputCol=\"resultIndex\")\ndf_full_oot = indexer.fit(df_full_oot).transform(df_full_oot)\n\ndf_full_oot = df_full_oot.fillna({\"result1\":\"none\"})\n\npipeline = Pipeline(stages=[indexer1, indexer2, indexer3])\ndf_full_oot = pipeline.fit(df_full_oot).transform(df_full_oot)\n\ndf_full_oot = df_full_oot.drop(\"result\", \"last\", \"protocol\", \"result1\")\n# display(df_full_test)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["df_train = df_full.select(df_full.columns[:])\ndf_test2 = df_full_test.select(df_full_test.columns[:])\ndf_oot2 = df_full_oot.select(df_full_oot.columns[:])"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["**One Hot Encoding (Sahil, Chris, Siddhant)**"],"metadata":{}},{"cell_type":"code","source":["#one hot encode the total_author_ntile columns\n#ohe = OneHotEncoderEstimator() #changed from OneHotEncoder\n#ohe.setInputCols([\"total_author_score_ntile\"])\n#ohe.setOutputCols([\"total_author_score_ntile_ohe\"])\n#df = ohe.fit(df).transform(df)\n\n#one hot encode the avg_author_ntile columns\n#ohe = OneHotEncoderEstimator() \n#ohe.setInputCols([\"avg_author_score_ntile\"])\n#ohe.setOutputCols([\"avg_author_score_ntile_ohe\"])\n#df = ohe.fit(df).transform(df)\n\nSI_subreddit = StringIndexer(inputCol=\"subreddit_rev\",outputCol=\"subreddit_rev_Index\")\nSI_DOW = StringIndexer(inputCol=\"DOW\",outputCol=\"DOW_Index\")\nSI_whitelist_status = StringIndexer(inputCol=\"whitelist_status\", outputCol=\"whitelist_statusIndex\")\nSI_subreddit_type = StringIndexer(inputCol=\"subreddit_type\", outputCol=\"subreddit_typeIndex\")\n\ndf_train = SI_subreddit.fit(df_train).transform(df_train)\ndf_train = SI_DOW.fit(df_train).transform(df_train)\ndf_train = SI_whitelist_status.fit(df_train).transform(df_train)\ndf_train = SI_subreddit_type.fit(df_train).transform(df_train)\n\ndf_train = df_train.fillna({\"lastIndex\":100})\nohe = OneHotEncoderEstimator()\nohe = OneHotEncoderEstimator(inputCols=[\"total_author_score_ntile\", \"avg_author_score_ntile\", \"subreddit_rev_Index\", \"DOW_Index\", \"whitelist_statusIndex\", \"subreddit_typeIndex\", \"resultIndex\", \"lastIndex\", \"protocolIndex\", \"result1Index\"],outputCols=[\"total_author_score_ntile_ohe\",\"avg_author_score_ntile_ohe\",\"subreddit_rev_ohe\", \"DOW_ohe\", \"whitelist_status_ohe\", \"subreddit_type_ohe\", \"result_ohe\", \"last_ohe\", \"protocol_ohe\", \"result1_ohe\"])\ndf_train = ohe.fit(df_train).transform(df_train)\n\n#model = df\n\n#print(model)\n#drop the author column\n#df_sahil_train = model.drop(\"author\", \"total_author_score_ntile\", \"avg_author_score_ntile\")\n#display(df_sahil_train.limit(4))"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["# testing data\npipeline = Pipeline(stages=[SI_subreddit, SI_DOW, SI_whitelist_status, SI_subreddit_type])\ndf_test2 = pipeline.fit(df_test2).transform(df_test2)\n\ndf_test2 = df_test2.fillna({\"lastIndex\":100})\ndf_test2 = ohe.fit(df_test2).transform(df_test2)\n\n# oot data\ndf_oot2 = pipeline.fit(df_oot2).transform(df_oot2)\n\ndf_oot2 = df_oot2.fillna({\"lastIndex\":100})\ndf_oot2 = ohe.fit(df_oot2).transform(df_oot2)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["**Finalize Dataset**"],"metadata":{}},{"cell_type":"code","source":["# drop columns - first pass\ncols_to_drop = [\"archived\",\"author_flair_background_color\",\"author_flair_css_class\",\"author_flair_richtext\",\"author_flair_text\",\"can_gild\",\"contest_mode\",\"distinguished\",\"edited\",\"gilded\",\"hidden\",\"hide_score\",\"id\",\"is_reddit_media_domain\",\"is_self\",\"is_video\",\"link_flair_css_class\",\"link_flair_richtext\",\"link_flair_text\",\"link_flair_text_color\",\"link_flair_type\",\"locked\",\"media\",\"num_crossposts\",\"parent_whitelist_status\",\"permalink\",\"post_hint\",\"preview\",\"retrieved_on\",\"rte_mode\",\"secure_media\",\"selftext\",\"send_replies\",\"spoiler\",\"stickied\",\"subreddit_id\",\"subreddit_name_prefixed\",\"suggested_sort\",\"thumbnail\",\"thumbnail_height\",\"thumbnail_width\"]\ndf_train_final = df_train.drop(*cols_to_drop)\ndf_test_final = df_test2.drop(*cols_to_drop)\ndf_oot_final = df_oot2.drop(*cols_to_drop)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["cols_to_drop2 = [\"author\",\"author_index2\",\"author_index\",\"avg_author_score_ntile\",\"total_author_score_ntile\",\"title\",\"author_flair_type\",\"is_crosspostable\",\"no_follow\",\"over_18\",\"subreddit_type\",\"url\",\"whitelist_status\",\"text\"]\ndf_train_final = df_train_final.drop(*cols_to_drop2)\ndf_test_final = df_test_final.drop(*cols_to_drop2)\ndf_oot_final = df_oot_final.drop(*cols_to_drop2)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["display(df_train_final)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["dbutils.fs.rm(\"/FileStore/my-stuff/df_train_final.parquet\",True)\ndbutils.fs.rm(\"/FileStore/my-stuff/df_test_final.parquet\",True)\ndbutils.fs.rm(\"/FileStore/my-stuff/df_oot_final.parquet\",True)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["df_train_final = df_train_final.withColumnRenamed(\"url length\",\"url_length\").withColumnRenamed(\"average da\",\"average_da\")\ndf_test_final = df_test_final.withColumnRenamed(\"url length\",\"url_length\").withColumnRenamed(\"average da\",\"average_da\")\ndf_oot_final = df_oot_final.withColumnRenamed(\"url length\",\"url_length\").withColumnRenamed(\"average da\",\"average_da\")\n\ndf_train_final.write.parquet(\"/FileStore/my-stuff/df_train_final.parquet\")\ndf_test_final.write.parquet(\"/FileStore/my-stuff/df_test_final.parquet\")\ndf_oot_final.write.parquet(\"/FileStore/my-stuff/df_oot_final.parquet\")"],"metadata":{},"outputs":[],"execution_count":101}],"metadata":{"name":"MIE1628 Project - Combined","notebookId":2440592797381846},"nbformat":4,"nbformat_minor":0}
